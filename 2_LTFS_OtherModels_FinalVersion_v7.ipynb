{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting up basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime as dt\n",
    "import gc\n",
    "import re\n",
    "# Visualization libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "#Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew \n",
    "#sklearn for pre-processing the data and prepping test data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from scipy.stats import ranksums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloc = 'C://Users//sandip.bhattacharjee//Desktop//AV Hackathons//input//'\n",
    "outloc = 'C://Users//sandip.bhattacharjee//Desktop//AV Hackathons//output//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((233154, 41), (112392, 40))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(dataloc + \"train.csv\")\n",
    "test = pd.read_csv(dataloc + \"test.csv\")\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nmiss(df):\n",
    "    df_na = (df.isnull().sum()/len(df))*100\n",
    "    df_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)[:30]\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :df_na})\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Employment.Type</th>\n",
       "      <td>3.285811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Missing Ratio\n",
       "Employment.Type       3.285811"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_miss = check_nmiss(train)\n",
    "train_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Employment.Type</th>\n",
       "      <td>3.063385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Missing Ratio\n",
       "Employment.Type       3.063385"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_miss = check_nmiss(test)\n",
    "test_miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, there are no missing values other than emplyment type that needs to be treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_1(df):\n",
    "    df['dob_date'] = pd.to_datetime(df['Date.of.Birth'])\n",
    "    df['age']= pd.to_datetime(\"now\") - df['dob_date']\n",
    "    df['age'] = df['age'].dt.days\n",
    "    df['disbursal_date'] = pd.to_datetime(df['DisbursalDate'])\n",
    "    #### Seasonal features ? Can there by seasonality #####\n",
    "    #df['disbursal_dayofweek'] = df['disbursal_date'].dt.dayofweek\n",
    "    #df['disbursal_month'] = df['disbursal_date'].dt.month\n",
    "    #df['disbursal_year'] = df['disbursal_date'].dt.year\n",
    "    \n",
    "    #######################################################\n",
    "    df['days_since_disbursal']= pd.to_datetime(\"now\") - df['disbursal_date']\n",
    "    df['days_since_disbursal'] = df['days_since_disbursal'].dt.days\n",
    "    df['tot_ch_years'] = df['CREDIT.HISTORY.LENGTH'].str.extract('^([^yrs]*).*', expand=False)\n",
    "    df['tot_ch_years'] = df['tot_ch_years'].astype('int')\n",
    "    df['tot_ch_months_']=df['CREDIT.HISTORY.LENGTH'].str.split(' ').str[1]\n",
    "    df['tot_ch_months'] = df['tot_ch_months_'].str.extract('^([^mon]*).*', expand=False)\n",
    "    df['tot_ch_months'] = df['tot_ch_months'].astype('int')\n",
    "    df['total_ch_months'] = df['tot_ch_years']*12 + df['tot_ch_months']\n",
    "    df.drop(['tot_ch_years','tot_ch_months_','tot_ch_months'],axis=1, inplace=True)\n",
    "    df['avg_acc_age_years'] = df['AVERAGE.ACCT.AGE'].str.extract('^([^yrs]*).*', expand=False)\n",
    "    df['avg_acc_age_years'] = df['avg_acc_age_years'].astype('int')\n",
    "    df['avg_acc_age_month_']=df['AVERAGE.ACCT.AGE'].str.split(' ').str[1]\n",
    "    df['avg_acc_age_month'] = df['avg_acc_age_month_'].str.extract('^([^mon]*).*', expand=False)\n",
    "    df['avg_acc_age_month'] = df['avg_acc_age_month'].astype('int')\n",
    "    df['total_acc_age_months'] = df['avg_acc_age_years']*12 + df['avg_acc_age_month']\n",
    "    df.drop(['avg_acc_age_years','avg_acc_age_month_','avg_acc_age_month','disbursal_date'],axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((233154, 46), (112392, 45))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=prep_data_1(train)\n",
    "test = prep_data_1(test)\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_feats(df):\n",
    "    df['total_idproof_given']= df['Aadhar_flag']+df['PAN_flag']+df['VoterID_flag']+df['Driving_flag']+df['Passport_flag']\n",
    "    df['pri_curr_balnc_pri_instal_ratio']=df['PRI.CURRENT.BALANCE']/df['PRIMARY.INSTAL.AMT']\n",
    "    df['overdue_ac_to_activeAc_Ratio']=df['PRI.OVERDUE.ACCTS']/df['PRI.ACTIVE.ACCTS']\n",
    "    df['Num_closed_loan_acc']=df['PRI.NO.OF.ACCTS']-df['PRI.OVERDUE.ACCTS']\n",
    "    df['disbursed_to_asset_cost']= df['disbursed_amount']/df['asset_cost']\n",
    "    df['ltv_diff_calc_ltv']=df['ltv']-df['disbursed_to_asset_cost']*100\n",
    "    ##\n",
    "    \n",
    "    df['employ_type_salaried'] = np.where(df['Employment.Type']=='Salaried',1,0)\n",
    "    df['employ_type_selfemplyd'] = np.where(df['Employment.Type']=='Self employed',1,0)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((233154, 54), (112392, 53))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= additional_feats(train)\n",
    "test = additional_feats(test)\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "CV score Current Fold: 0.62848 \n",
      "fold 1\n",
      "CV score Current Fold: 0.63225 \n",
      "fold 2\n",
      "CV score Current Fold: 0.62997 \n",
      "fold 3\n",
      "CV score Current Fold: 0.62937 \n",
      "fold 4\n",
      "CV score Current Fold: 0.62568 \n"
     ]
    }
   ],
   "source": [
    "train_columns = [c for c in train.columns if c not in ['UniqueID', 'loan_default','dob_date',\n",
    "                                                       'disbursal_date','Date.of.Birth',\n",
    "                                                       'Employment.Type','DisbursalDate','PERFORM_CNS.SCORE.DESCRIPTION',\n",
    "                                                       'AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n",
    "                                                      'PERFORM_CNS.SCORE.DESCRIPTION',\n",
    "                                                      'PERFORM_CNS.SCORE.DESCRIPTION','disbursal_year',\n",
    "                                                      'MobileNo_Avl_Flag','Passport_flag']]\n",
    "target = train['loan_default']\n",
    "test_df = test[train_columns]\n",
    "folds = StratifiedKFold(n_splits=5,shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(train))\n",
    "#predictions_nn = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "all_cols = train.columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['loan_default'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    train_set = train.iloc[trn_idx][all_cols]\n",
    "    val_set = train.iloc[val_idx][all_cols]\n",
    "    test_df = test[train_columns]\n",
    "    \n",
    "    train_set.fillna(0,inplace=True)\n",
    "    val_set.fillna(0,inplace=True)\n",
    "    \n",
    "    test_df.fillna(0,inplace=True)\n",
    "    \n",
    "    X_train,y_train= train_set[train_columns],train_set['loan_default']\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    del train_set\n",
    "    gc.collect()\n",
    "    X_val,y_val = val_set[train_columns],val_set['loan_default']\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "    del val_set\n",
    "    gc.collect()\n",
    "    \n",
    "    test_df_std = test_df[train_columns]\n",
    "    tst_df_shape = test_df_std.shape[0]\n",
    "    test_df_std = scaler.fit_transform(test_df_std)\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression(random_state=0,penalty='l2', solver='lbfgs',fit_intercept=True,multi_class='ovr',max_iter=1000).fit(X_train,y_train)\n",
    "         \n",
    "    \n",
    "    oof = clf.predict_proba(X_val)\n",
    "    print(\"CV score Current Fold: {:<8.5f}\".format(roc_auc_score(y_val, oof[:,1])))\n",
    "    if fold_ == 0:\n",
    "        predictions_lr = clf.predict_proba(test_df_std) \n",
    "    elif fold_ >0:\n",
    "        predictions_lr += clf.predict_proba(test_df_std) / folds.n_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4360083 , 0.51573698, 0.37350353, ..., 0.25983445, 0.43988682,\n",
       "       0.23360291])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_logistic = predictions_lr[:,1]\n",
    "pred_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"UniqueID\": test.UniqueID.values})\n",
    "sub[\"loan_default\"] = pred_logistic\n",
    "sub.to_csv(outloc + \"final_submission_logistics_04212019_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "CV score Current Fold: 0.60838 \n",
      "fold 1\n",
      "CV score Current Fold: 0.60969 \n",
      "fold 2\n",
      "CV score Current Fold: 0.60994 \n",
      "fold 3\n",
      "CV score Current Fold: 0.60581 \n",
      "fold 4\n",
      "CV score Current Fold: 0.60421 \n",
      "fold 5\n",
      "CV score Current Fold: 0.60637 \n",
      "fold 6\n",
      "CV score Current Fold: 0.61658 \n"
     ]
    }
   ],
   "source": [
    "train_columns = [c for c in train.columns if c not in ['UniqueID', 'loan_default','dob_date',\n",
    "                                                       'disbursal_date','Date.of.Birth',\n",
    "                                                       'Employment.Type','DisbursalDate','PERFORM_CNS.SCORE.DESCRIPTION',\n",
    "                                                       'AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n",
    "                                                      'PERFORM_CNS.SCORE.DESCRIPTION',\n",
    "                                                      'PERFORM_CNS.SCORE.DESCRIPTION','disbursal_year',\n",
    "                                                      'MobileNo_Avl_Flag','Passport_flag']]\n",
    "target = train['loan_default']\n",
    "test_df = test[train_columns]\n",
    "folds = StratifiedKFold(n_splits=7,shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(train))\n",
    "all_cols = train.columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['loan_default'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    train_set = train.iloc[trn_idx][all_cols]\n",
    "    val_set = train.iloc[val_idx][all_cols]\n",
    "    test_df = test[train_columns]\n",
    "    \n",
    "    train_set.fillna(0,inplace=True)\n",
    "    val_set.fillna(0,inplace=True)\n",
    "    \n",
    "    test_df.fillna(0,inplace=True)\n",
    "    \n",
    "    X_train,y_train= train_set[train_columns],train_set['loan_default']\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    del train_set\n",
    "    gc.collect()\n",
    "    X_val,y_val = val_set[train_columns],val_set['loan_default']\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "    del val_set\n",
    "    gc.collect()\n",
    "    \n",
    "    test_df_std = test_df[train_columns]\n",
    "    tst_df_shape = test_df_std.shape[0]\n",
    "    test_df_std = scaler.fit_transform(test_df_std)\n",
    "    \n",
    "    \n",
    "    #clf = GaussianNB( var_smoothing=0.005)\n",
    "    clf = BernoulliNB(alpha=1250)\n",
    "    clf.fit(X_train,y_train)\n",
    "         \n",
    "    \n",
    "    oof = clf.predict_proba(X_val)\n",
    "    print(\"CV score Current Fold: {:<8.5f}\".format(roc_auc_score(y_val, oof[:,1])))\n",
    "    if fold_ == 0:\n",
    "        predictions_nb = clf.predict_proba(test_df_std) \n",
    "    elif fold_ >0:\n",
    "        predictions_nb += clf.predict_proba(test_df_std) / folds.n_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34441687, 0.39082651, 0.60938154, ..., 0.25832192, 0.36941329,\n",
       "       0.25413693])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nb = predictions_nb[:,1]\n",
    "pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"UniqueID\": test.UniqueID.values})\n",
    "sub[\"loan_default\"] = pred_nb\n",
    "sub.to_csv(outloc + \"final_submission_NB_localCV_605_7fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = [c for c in train.columns if c not in ['UniqueID', 'loan_default','dob_date',\n",
    "                                                       'disbursal_date','Date.of.Birth',\n",
    "                                                       'Employment.Type','DisbursalDate','PERFORM_CNS.SCORE.DESCRIPTION',\n",
    "                                                       'AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n",
    "                                                      'PERFORM_CNS.SCORE.DESCRIPTION',\n",
    "                                                      'PERFORM_CNS.SCORE.DESCRIPTION']]\n",
    "target = train['loan_default']\n",
    "test_df = test[train_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's auc: 0.682924\tvalid_1's auc: 0.665181\n",
      "[1000]\ttraining's auc: 0.692679\tvalid_1's auc: 0.669479\n",
      "[1500]\ttraining's auc: 0.700833\tvalid_1's auc: 0.672148\n",
      "[2000]\ttraining's auc: 0.708067\tvalid_1's auc: 0.674068\n",
      "[2500]\ttraining's auc: 0.714557\tvalid_1's auc: 0.675421\n",
      "[3000]\ttraining's auc: 0.720906\tvalid_1's auc: 0.676258\n",
      "[3500]\ttraining's auc: 0.726828\tvalid_1's auc: 0.676993\n",
      "[4000]\ttraining's auc: 0.732591\tvalid_1's auc: 0.677661\n",
      "[4500]\ttraining's auc: 0.737977\tvalid_1's auc: 0.678127\n",
      "[5000]\ttraining's auc: 0.74317\tvalid_1's auc: 0.678418\n",
      "[5500]\ttraining's auc: 0.748027\tvalid_1's auc: 0.678706\n",
      "[6000]\ttraining's auc: 0.752774\tvalid_1's auc: 0.678947\n",
      "[6500]\ttraining's auc: 0.757379\tvalid_1's auc: 0.679086\n",
      "[7000]\ttraining's auc: 0.761881\tvalid_1's auc: 0.679083\n",
      "[7500]\ttraining's auc: 0.766133\tvalid_1's auc: 0.679071\n",
      "Early stopping, best iteration is:\n",
      "[6885]\ttraining's auc: 0.760846\tvalid_1's auc: 0.679151\n",
      "fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's auc: 0.68337\tvalid_1's auc: 0.660643\n",
      "[1000]\ttraining's auc: 0.693426\tvalid_1's auc: 0.665234\n",
      "[1500]\ttraining's auc: 0.701597\tvalid_1's auc: 0.667741\n",
      "[2000]\ttraining's auc: 0.708864\tvalid_1's auc: 0.669488\n",
      "[2500]\ttraining's auc: 0.715405\tvalid_1's auc: 0.670588\n",
      "[3000]\ttraining's auc: 0.721744\tvalid_1's auc: 0.671332\n",
      "[3500]\ttraining's auc: 0.72766\tvalid_1's auc: 0.671826\n",
      "[4000]\ttraining's auc: 0.733377\tvalid_1's auc: 0.672175\n",
      "[4500]\ttraining's auc: 0.738867\tvalid_1's auc: 0.672522\n",
      "[5000]\ttraining's auc: 0.744047\tvalid_1's auc: 0.672684\n",
      "[5500]\ttraining's auc: 0.748939\tvalid_1's auc: 0.672782\n",
      "[6000]\ttraining's auc: 0.753665\tvalid_1's auc: 0.672842\n",
      "[6500]\ttraining's auc: 0.75825\tvalid_1's auc: 0.672835\n",
      "[7000]\ttraining's auc: 0.762722\tvalid_1's auc: 0.67289\n",
      "[7500]\ttraining's auc: 0.76703\tvalid_1's auc: 0.672778\n",
      "Early stopping, best iteration is:\n",
      "[6976]\ttraining's auc: 0.762492\tvalid_1's auc: 0.672896\n",
      "fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's auc: 0.682256\tvalid_1's auc: 0.666818\n",
      "[1000]\ttraining's auc: 0.692333\tvalid_1's auc: 0.671857\n",
      "[1500]\ttraining's auc: 0.700607\tvalid_1's auc: 0.674672\n",
      "[2000]\ttraining's auc: 0.707968\tvalid_1's auc: 0.676715\n",
      "[2500]\ttraining's auc: 0.714659\tvalid_1's auc: 0.677981\n",
      "[3000]\ttraining's auc: 0.720997\tvalid_1's auc: 0.678849\n",
      "[3500]\ttraining's auc: 0.726966\tvalid_1's auc: 0.679347\n",
      "[4000]\ttraining's auc: 0.732647\tvalid_1's auc: 0.679641\n",
      "[4500]\ttraining's auc: 0.738174\tvalid_1's auc: 0.679864\n",
      "[5000]\ttraining's auc: 0.743358\tvalid_1's auc: 0.680042\n",
      "[5500]\ttraining's auc: 0.748322\tvalid_1's auc: 0.68018\n",
      "[6000]\ttraining's auc: 0.753069\tvalid_1's auc: 0.680284\n",
      "[6500]\ttraining's auc: 0.757772\tvalid_1's auc: 0.680294\n",
      "[7000]\ttraining's auc: 0.762335\tvalid_1's auc: 0.680344\n",
      "[7500]\ttraining's auc: 0.766674\tvalid_1's auc: 0.680256\n",
      "Early stopping, best iteration is:\n",
      "[6590]\ttraining's auc: 0.758579\tvalid_1's auc: 0.680356\n",
      "fold 3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's auc: 0.683245\tvalid_1's auc: 0.661232\n",
      "[1000]\ttraining's auc: 0.69313\tvalid_1's auc: 0.665566\n",
      "[1500]\ttraining's auc: 0.701216\tvalid_1's auc: 0.668093\n",
      "[2000]\ttraining's auc: 0.708473\tvalid_1's auc: 0.669894\n",
      "[2500]\ttraining's auc: 0.714997\tvalid_1's auc: 0.671127\n",
      "[3000]\ttraining's auc: 0.721299\tvalid_1's auc: 0.671924\n",
      "[3500]\ttraining's auc: 0.727189\tvalid_1's auc: 0.672702\n",
      "[4000]\ttraining's auc: 0.732895\tvalid_1's auc: 0.673369\n",
      "[4500]\ttraining's auc: 0.7383\tvalid_1's auc: 0.673812\n",
      "[5000]\ttraining's auc: 0.743394\tvalid_1's auc: 0.674146\n",
      "[5500]\ttraining's auc: 0.748291\tvalid_1's auc: 0.674318\n",
      "[6000]\ttraining's auc: 0.752927\tvalid_1's auc: 0.67443\n",
      "[6500]\ttraining's auc: 0.757524\tvalid_1's auc: 0.674585\n",
      "[7000]\ttraining's auc: 0.761923\tvalid_1's auc: 0.674782\n",
      "[7500]\ttraining's auc: 0.766186\tvalid_1's auc: 0.674783\n",
      "[8000]\ttraining's auc: 0.770351\tvalid_1's auc: 0.674835\n",
      "[8500]\ttraining's auc: 0.77443\tvalid_1's auc: 0.674851\n",
      "[9000]\ttraining's auc: 0.778348\tvalid_1's auc: 0.674944\n",
      "[9500]\ttraining's auc: 0.782158\tvalid_1's auc: 0.674981\n",
      "[10000]\ttraining's auc: 0.78594\tvalid_1's auc: 0.674902\n",
      "[10500]\ttraining's auc: 0.78947\tvalid_1's auc: 0.67495\n",
      "Early stopping, best iteration is:\n",
      "[9593]\ttraining's auc: 0.782816\tvalid_1's auc: 0.67502\n",
      "fold 4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's auc: 0.682492\tvalid_1's auc: 0.665424\n",
      "[1000]\ttraining's auc: 0.692462\tvalid_1's auc: 0.669532\n",
      "[1500]\ttraining's auc: 0.700633\tvalid_1's auc: 0.672283\n",
      "[2000]\ttraining's auc: 0.707971\tvalid_1's auc: 0.673743\n",
      "[2500]\ttraining's auc: 0.714647\tvalid_1's auc: 0.674814\n",
      "[3000]\ttraining's auc: 0.720998\tvalid_1's auc: 0.675449\n",
      "[3500]\ttraining's auc: 0.726949\tvalid_1's auc: 0.676021\n",
      "[4000]\ttraining's auc: 0.732729\tvalid_1's auc: 0.676592\n",
      "[4500]\ttraining's auc: 0.738089\tvalid_1's auc: 0.676977\n",
      "[5000]\ttraining's auc: 0.743268\tvalid_1's auc: 0.677227\n",
      "[5500]\ttraining's auc: 0.748225\tvalid_1's auc: 0.677434\n",
      "[6000]\ttraining's auc: 0.75298\tvalid_1's auc: 0.677488\n",
      "[6500]\ttraining's auc: 0.757537\tvalid_1's auc: 0.677608\n",
      "[7000]\ttraining's auc: 0.762072\tvalid_1's auc: 0.677675\n",
      "[7500]\ttraining's auc: 0.766373\tvalid_1's auc: 0.677752\n",
      "[8000]\ttraining's auc: 0.770507\tvalid_1's auc: 0.677774\n",
      "[8500]\ttraining's auc: 0.774606\tvalid_1's auc: 0.677762\n",
      "Early stopping, best iteration is:\n",
      "[7852]\ttraining's auc: 0.769285\tvalid_1's auc: 0.677829\n",
      "fold 5\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's auc: 0.683353\tvalid_1's auc: 0.658291\n",
      "[1000]\ttraining's auc: 0.693388\tvalid_1's auc: 0.663405\n",
      "[1500]\ttraining's auc: 0.701554\tvalid_1's auc: 0.666428\n",
      "[2000]\ttraining's auc: 0.708789\tvalid_1's auc: 0.66829\n",
      "[2500]\ttraining's auc: 0.715352\tvalid_1's auc: 0.669538\n",
      "[3000]\ttraining's auc: 0.721636\tvalid_1's auc: 0.670547\n",
      "[3500]\ttraining's auc: 0.727574\tvalid_1's auc: 0.6712\n",
      "[4000]\ttraining's auc: 0.733333\tvalid_1's auc: 0.671774\n",
      "[4500]\ttraining's auc: 0.738739\tvalid_1's auc: 0.672279\n",
      "[5000]\ttraining's auc: 0.743862\tvalid_1's auc: 0.672613\n",
      "[5500]\ttraining's auc: 0.748784\tvalid_1's auc: 0.672911\n",
      "[6000]\ttraining's auc: 0.753521\tvalid_1's auc: 0.673024\n",
      "[6500]\ttraining's auc: 0.758101\tvalid_1's auc: 0.673157\n",
      "[7000]\ttraining's auc: 0.76256\tvalid_1's auc: 0.67319\n",
      "[7500]\ttraining's auc: 0.766831\tvalid_1's auc: 0.673314\n",
      "[8000]\ttraining's auc: 0.771039\tvalid_1's auc: 0.673345\n",
      "[8500]\ttraining's auc: 0.775122\tvalid_1's auc: 0.673422\n",
      "[9000]\ttraining's auc: 0.779024\tvalid_1's auc: 0.673495\n",
      "[9500]\ttraining's auc: 0.782849\tvalid_1's auc: 0.673559\n",
      "[10000]\ttraining's auc: 0.78664\tvalid_1's auc: 0.673494\n",
      "Early stopping, best iteration is:\n",
      "[9435]\ttraining's auc: 0.782317\tvalid_1's auc: 0.673586\n",
      "fold 6\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's auc: 0.682801\tvalid_1's auc: 0.665561\n",
      "[1000]\ttraining's auc: 0.6928\tvalid_1's auc: 0.669544\n",
      "[1500]\ttraining's auc: 0.700888\tvalid_1's auc: 0.671821\n",
      "[2000]\ttraining's auc: 0.708108\tvalid_1's auc: 0.673419\n",
      "[2500]\ttraining's auc: 0.714702\tvalid_1's auc: 0.674667\n",
      "[3000]\ttraining's auc: 0.721068\tvalid_1's auc: 0.67547\n",
      "[3500]\ttraining's auc: 0.727009\tvalid_1's auc: 0.676131\n",
      "[4000]\ttraining's auc: 0.732716\tvalid_1's auc: 0.67663\n",
      "[4500]\ttraining's auc: 0.738144\tvalid_1's auc: 0.676979\n",
      "[5000]\ttraining's auc: 0.743335\tvalid_1's auc: 0.677261\n",
      "[5500]\ttraining's auc: 0.748316\tvalid_1's auc: 0.677452\n",
      "[6000]\ttraining's auc: 0.753051\tvalid_1's auc: 0.677522\n",
      "[6500]\ttraining's auc: 0.75774\tvalid_1's auc: 0.677512\n",
      "[7000]\ttraining's auc: 0.762253\tvalid_1's auc: 0.67751\n",
      "[7500]\ttraining's auc: 0.766516\tvalid_1's auc: 0.677465\n",
      "Early stopping, best iteration is:\n",
      "[6734]\ttraining's auc: 0.759858\tvalid_1's auc: 0.677569\n",
      "CV score: 0.67659 \n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 65,\n",
    "         'min_sum_hessian_in_leaf': 20.0,\n",
    "         'min_data_in_leaf': 80, \n",
    "         'objective':'binary',\n",
    "         'tree_learner': 'feature',   \n",
    "         'boost_from_average':'true',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.004,\n",
    "         \"min_child_samples\": 50,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.15,\n",
    "         \"bagging_freq\": 5,\n",
    "         \"bagging_fraction\": 0.7 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'auc',\n",
    "         'reg_alpha': 0.35, \n",
    "         'reg_lambda': 0.25,\n",
    "         \"verbosity\": -1,\n",
    "         #categorical_feature': 'auto',\n",
    "         'is_unbalanced':'True',\n",
    "         \"nthread\": 6,\n",
    "         \"random_state\": 99999}\n",
    "folds = StratifiedKFold(n_splits=7,shuffle=True, random_state=99999)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['loan_default'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 100000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 1000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test_df[train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"UniqueID\": test.UniqueID.values})\n",
    "sub[\"loan_default\"] = predictions\n",
    "sub.to_csv(outloc + \"final_submission_LightGBM_CV067659_04212019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing other files for blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_outloc = \"C://Users//sandip.bhattacharjee//Desktop//AV Hackathons//final_submission//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred = pd.read_csv(fin_outloc + \"fianl_submission_NN_04212019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred_array = np.array(nn_pred['loan_default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_pred = predictions*0.92+0.08*nn_pred_array+0*pred_logistic+0*pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25113277, 0.2766064 , 0.24333842, ..., 0.16854025, 0.24677827,\n",
       "       0.1152196 ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blended_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"UniqueID\": test.UniqueID.values})\n",
    "sub[\"loan_default\"] = blended_pred\n",
    "sub.to_csv(fin_outloc + \"final_blended_sub_04212019_v7.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
